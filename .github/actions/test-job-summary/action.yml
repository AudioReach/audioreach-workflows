name: LAVA test job summary
description: "Download LAVA job artifacts, build a Markdown summary, and upload it as a workflow artifact"

inputs:
  build_id:
    description: "ID of the workflow from which build URL will be downloaded"
    required: true
  gh_token:
    description: "Github token. It's required to download artifacts from the workflow"
    required: true
  prefix:
    description: "Prefix for the compressed file that is uploaded as workflow artifact"
    default: test-job
  summary_file_name:
    description: "File name that is used to save test job summary"
    default: step-summary.txt

outputs:
  artifact_id:
    description: "ID of the uploaded artifact"
    value: ${{ steps.upload-summary.outputs.artifact-id }}

runs:
  using: "composite"

  steps:

    - name: Download Artifacts
        # This step is safe because inputs are not interpolated into shell.
      uses: actions/download-artifact@v6
      with:
        github-token: ${{ inputs.gh_token }}
        run-id: ${{ inputs.build_id }}
        path: artifacts
        pattern: test-job*
        merge-multiple: true

    - name: List files
      shell: bash
      env:
        WORKSPACE: "${{ github.workspace }}"
      run: |
        set -euo pipefail
        echo "Workspace: ${WORKSPACE}"
        ls -R -- "${WORKSPACE}"

    - name: Publish Test Job Details
      shell: bash
      env:
        WORKSPACE: "${{ github.workspace }}"
        PREFIX: "${{ inputs.prefix }}"
        SUMMARY_FILE: "${{ inputs.summary_file_name }}"
      run: |
        set -euo pipefail

        # --- Validate inputs (defense-in-depth) ---
        safe_name_re='^[A-Za-z0-9._-]+$'
        if ! printf '%s' "${PREFIX}" | grep -Eq "${safe_name_re}"; then
          echo "Invalid prefix: ${PREFIX}" >&2
          exit 2
        fi
        if ! printf '%s' "${SUMMARY_FILE}" | grep -Eq "${safe_name_re}"; then
          echo "Invalid summary_file_name: ${SUMMARY_FILE}" >&2
          exit 2
        fi

        echo "Workspace: ${WORKSPACE}"
        echo "Summary file: ${SUMMARY_FILE}"
        echo "Prefix: ${PREFIX}"
        echo "Fetching test jobs..."

        # --- Python: process per-job JSONs and produce a summary table ---
        
        python3 << 'PYCODE'
        import json
        import os
        import sys
        import requests
        from collections import defaultdict
        import urllib3
        
        # Disable SSL warnings (kept as in your original)
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        def fetch_job_details(job_id):
        	url = f"https://lava.infra.foundries.io/api/v0.2/jobs/{job_id}/"
        	try:
        		response = requests.get(url, timeout=30, verify=False)
        		response.raise_for_status()
        		return response.json()
        	except Exception as e:
        		print(f"Error fetching job {job_id}: {e}", file=sys.stderr)
        		return None
        
        def fetch_job_suites(job_id):
        	url = f"https://lava.infra.foundries.io/api/v0.2/jobs/{job_id}/suites/"
        	try:
        		response = requests.get(url, timeout=30, verify=False)
        		response.raise_for_status()
        		return response.json()
        	except Exception as e:
        		print(f"Error fetching suites for job {job_id}: {e}", file=sys.stderr)
        		return None
        
        def fetch_suite_tests(job_id, suite_id):
        	url = f"https://lava.infra.foundries.io/api/v0.2/jobs/{job_id}/suites/{suite_id}/tests/"
        	try:
        		response = requests.get(url, timeout=30, verify=False)
        		response.raise_for_status()
        		return response.json()
        	except Exception as e:
        		print(f"Error fetching tests for job {job_id}, suite {suite_id}: {e}", file=sys.stderr)
        		return None
        
        def process_test_job(test_job_file):
        	print(f"Processing: {test_job_file}")
        	
        	with open(test_job_file, 'r') as f:
        		job_data = json.load(f)
        	
        	job_id = job_data.get('id')
        	if not job_id:
        		return None
        	
        	print(f"  Job ID: {job_id}")
        	
        	job_details = fetch_job_details(job_id)
        	if not job_details:
        		return None
        	
        	job_health = job_details.get('health')
        	job_state = job_details.get('state')
        	job_device_type = job_details.get('requested_device_type')
        	job_url = f"https://lava.infra.foundries.io/results/{job_id}"
        	
        	print(f"  State: {job_state}, Health: {job_health}, Device: {job_device_type}")
        	
        	test_results = {}
        	
        	if job_state == "Finished" and job_health == "Complete":
        		suites_data = fetch_job_suites(job_id)
        		if not suites_data:
        			return None
        		
        		suites = suites_data.get('results', [])
        		print(f"  Found {len(suites)} suites")
        		
        		for suite in suites:
        			suite_name = suite.get('name')
        			suite_id = suite.get('id')
        			
        			print(f"    Processing suite: {suite_name} (ID: {suite_id})")
        			
        			suite_tests = fetch_suite_tests(job_id, suite_id)
        			if not suite_tests:
        				print(f"      No tests data returned for suite {suite_name}")
        				continue
        			
        			tests = suite_tests.get('results', [])
        			print(f"      Found {len(tests)} tests in suite {suite_name}")
        			
        			if suite_name != "lava":
        				for test in tests:
        					test_name = test.get('name')
        					test_result = test.get('result')
        					print(f"        Test: {test_name} = {test_result}")
        					if test_name:
        						# Use suite_name as prefix to make test names unique
        						full_test_name = f"{suite_name}.{test_name}"
        						test_results[full_test_name] = {
        							'result': test_result,
        							'url': job_url
        						}
        			else:
        				test_results['boot'] = {
        					'result': 'pass',
        					'url': job_url
        				}
        	else:
        		print(f"  Skipping - Job not finished or not complete")
        	
        	print(f"  Total test results collected: {len(test_results)}")
        	return {
        		'device_type': job_device_type,
        		'test_results': test_results,
        		'job_id': job_id
        	}
        
        def generate_summary_table(job_results, summary_file):
        	if not job_results:
        		print("No results to display")
        		return
        	
        	all_test_names = set()
        	for job_file, job_info in job_results:
        		all_test_names.update(job_info['test_results'].keys())
        	
        	test_names = sorted(all_test_names)
        	
        	lines = []
        	lines.append("# LAVA Test Job Results Summary")
        	lines.append("")
        	lines.append(f"**Total Jobs Processed:** {len(job_results)}")
        	lines.append("")
        	
        	# Header
        	header = "| Test |"
        	for job_file, job_info in job_results:
        		device_type = job_info['device_type']
        		header += f" {device_type} |"
        	lines.append(header)
        	
        	# Separator
        	separator = "| ---- |"
        	for _ in job_results:
        		separator += " ---- |"
        	lines.append(separator)
        	
        	# Data rows
        	for test_name in test_names:
        		row = f"| {test_name} |"
        		
        		for job_file, job_info in job_results:
        			test_results = job_info['test_results']
        			job_url = f"https://lava.infra.foundries.io/results/{job_info['job_id']}"
        			
        			if test_name in test_results:
        				result = test_results[test_name].get('result', '')
        				test_url = test_results[test_name].get('url', job_url)
        				
        				if result == 'pass':
        					checkmark = ":white_check_mark:"
        				elif result == 'fail':
        					checkmark = ":x:"
        				elif result == 'skip':
        					checkmark = ":warning:"
        				else:
        					checkmark = ":no_entry_sign:"
        				
        				# Make the checkmark a clickable link
        				row += f" [{checkmark}]({test_url}) |"
        			else:
        				row += f" :no_entry_sign: |"
        		
        		lines.append(row)
        	
        	# Print to console
        	for line in lines:
        		print(line)
        	
        	# Write to file
        	with open(summary_file, 'w') as f:
        		for line in lines:
        			f.write(line + '\n')
        	
        	print(f"\nSummary written to: {summary_file}")
        
        # Main execution
        WORKSPACE = os.environ.get("WORKSPACE", "")
        PREFIX = os.environ.get("PREFIX", "")
        SUMMARY_FILE = os.environ.get("SUMMARY_FILE", "step-summary.txt")
        
        print(f"Workspace: {workspace}")
        print(f"Looking for files matching: {prefix}*.json")
        print("="*80)
        
        # Find test job files
        test_job_files = []
        for root, dirs, files in os.walk(workspace):
        	for file in files:
        		if file.startswith(prefix) and file.endswith('.json'):
        			test_job_files.append(os.path.join(root, file))
        
        if not test_job_files:
        	print(f"No test job files found")
        	sys.exit(1)
        
        print(f"Found {len(test_job_files)} test job file(s)\n")
        
        # Process jobs
        job_results = []
        for test_job_file in test_job_files:
        	result = process_test_job(test_job_file)
        	if result:
        		job_results.append((test_job_file, result))
        	print()
        
        # Generate summary
        generate_summary_table(job_results, summary_file)
        PYCODE

        # --- Append the "All jobs summary" table (safe & quoted) ---
        # Ensure jq and curl exist (optional quick check)
        if ! command -v jq >/dev/null 2>&1; then
          echo "jq is required but not found on PATH." >&2
          exit 1
        fi
        if ! command -v curl >/dev/null 2>&1; then
          echo "curl is required but not found on PATH." >&2
          exit 1
        fi

        {
          printf '### All jobs summary\n'
          printf '\n'
          printf '| Job ID | Device | State | Health |\n'
          printf '| ---- | ---- | ---- | ---- |\n'
        } >> "${SUMMARY_FILE}"

        # Use NUL-delimited iteration for safety with filenames
        find "${WORKSPACE}" -type f -name "${PREFIX}*.json" -print0 \
        | while IFS= read -r -d '' TESTJOB; do
            JOB_ID="$(jq -r '.id' -- "${TESTJOB}")"
            [ -n "${JOB_ID}" ] || continue
            JOB_URL="https://lava.infra.foundries.io/results/${JOB_ID}"
            JOB_DETAILS="$(curl -fsSL --retry 3 --retry-delay 2 --max-time 30 "https://lava.infra.foundries.io/api/v0.2/jobs/${JOB_ID}/" || true)"
            if [ -z "${JOB_DETAILS}" ]; then
              # Print a row indicating fetch failure
              printf '| [%s](%s) | %s | %s | %s |\n' "${JOB_ID}" "${JOB_URL}" "?" "?" "fetch_error" >> "${SUMMARY_FILE}"
              continue
            fi
            JOB_HEALTH="$(printf '%s' "${JOB_DETAILS}" | jq -r '.health // "?"')"
            JOB_STATE="$(printf '%s' "${JOB_DETAILS}" | jq -r '.state // "?"')"
            JOB_DEVICE_TYPE="$(printf '%s' "${JOB_DETAILS}" | jq -r '.requested_device_type // "?"')"
            printf '| [%s](%s) | %s | %s | %s |\n' "${JOB_ID}" "${JOB_URL}" "${JOB_DEVICE_TYPE}" "${JOB_STATE}" "${JOB_HEALTH}" >> "${SUMMARY_FILE}"
          done

    - name: Upload summary
      id: upload-summary
      uses: actions/upload-artifact@v6
      with:
        path: ${{ inputs.summary_file_name }}
        name: ${{ inputs.summary_file_name }}
